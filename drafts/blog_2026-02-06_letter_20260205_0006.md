---
title: "Building a Cross-Platform Voice AI App: Six Phases from Desktop to Mobile"
date: 2026-02-06
tags: [tauri, nextjs, mobile-development, voice-ai, cross-platform, rust, typescript]
excerpt: "How we transformed Aurus Voice Intelligence from a desktop-only app into a mobile-ready cross-platform architecture using Tauri v2, Next.js, and platform-aware abstractions—complete with the lessons learned along the way."
---

# Building a Cross-Platform Voice AI App: Six Phases from Desktop to Mobile

Last night, I wrapped up the sixth phase of migrating **Aurus Voice Intelligence** from a desktop-only application to a truly cross-platform, mobile-ready architecture. With 52 passing tests, zero build warnings, and a clean commit, it felt like the right moment to share what this journey looked like—and what I learned about building platform-aware applications.

## The Challenge

Voice intelligence applications have unique requirements: they need low-latency audio capture, secure credential storage, and often benefit from on-device AI processing. Building this for desktop is one thing; making it work seamlessly across desktop *and* mobile platforms is another challenge entirely.

Our goal was to migrate from a desktop-only Tauri v1 setup to **Tauri v2** with mobile support, using **Next.js** for the frontend and maintaining a clean separation between platform-specific and shared code.

## The Architecture: Six Phases

### Phase 1: Platform Abstraction Layer

The foundation of any cross-platform app is a solid abstraction layer. We created `src-tauri/src/platform/` with trait-based interfaces for platform-specific functionality:

```rust
// Platform-agnostic secure storage trait
pub trait SecureStorage {
    fn set(&self, key: &str, value: &str) -> Result<()>;
    fn get(&self, key: &str) -> Result<Option<String>>;
    fn delete(&self, key: &str) -> Result<()>;
}
```

Each platform gets its own implementation:
- **macOS**: Native Keychain
- **iOS**: Keychain Services
- **Android**: EncryptedSharedPreferences
- **Windows**: Credential Manager
- **Linux**: Secret Service API

This pattern meant our application code could remain platform-agnostic while leveraging native security features where available.

### Phase 2: Secure Secrets Management

With the abstraction layer in place, we rewrote `secrets.rs` to use platform detection and dispatch to the appropriate backend:

```rust
pub fn get_current_platform() -> Platform {
    #[cfg(target_os = "macos")]
    return Platform::MacOS;
    
    #[cfg(target_os = "ios")]
    return Platform::IOS;
    
    // ... and so on
}
```

This phase included fixing edge cases in Keychain error handling—those cryptic `OSStatus` codes finally made sense.

### Phase 3: Audio Capture Abstraction

Audio capture is where platform differences really shine. Desktop platforms can use [CPAL](https://github.com/RustAudio/cpal) for low-latency native audio, but mobile browsers require the Web Audio API.

We created an `AudioCapture` trait and two implementations:

```typescript
// Frontend hook for mobile
export function useWebAudioCapture() {
  const { platform } = usePlatform();
  
  const startCapture = async () => {
    if (platform === 'mobile') {
      // Use MediaRecorder + Web Audio API
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      // Process and send via Tauri IPC to Deepgram
    }
  };
  
  return { startCapture, stopCapture, isRecording };
}
```

The beauty of this approach: desktop users get native performance, mobile users get a progressive web experience, and the UI component doesn't need to care about the difference.

### Phase 4: On-Device AI with Web Workers

For privacy and offline capability, we integrated [Transformers.js](https://huggingface.co/docs/transformers.js) for on-device transcription:

```typescript
// ai-worker.ts
import { pipeline } from '@xenova/transformers';

let transcriber;

self.addEventListener('message', async (e) => {
  if (e.data.type === 'initialize') {
    transcriber = await pipeline(
      'automatic-speech-recognition',
      'Xenova/whisper-tiny.en'
    );
  }
  // Handle transcription requests...
});
```

This runs entirely in a Web Worker, keeping the main thread responsive while processing audio locally.

### Phase 5 & 6: Integration and Testing

The final phases involved:
- Integrating `useWebAudioCapture` into the VoiceInput component
- Platform-gating features (like the "save recording" button—mobile browsers don't have native file dialogs)
- Writing comprehensive tests for all platform detection hooks
- Cleaning up warnings (goodbye, unused `calculate_energy` function!)

## Lessons Learned: The Pain Points

### 1. **jsdom Doesn't Have Web APIs**

When testing `useLocalAIAvailable`, I assumed Jest's jsdom environment would provide a `Worker` constructor. It doesn't.

```typescript
// The fix: explicit mocking
beforeEach(() => {
  (globalThis as any).Worker = class MockWorker {
    postMessage() {}
    addEventListener() {}
  };
});

afterEach(() => {
  delete (globalThis as any).Worker;
});
```

**Takeaway**: jsdom lacks `Worker`, `AudioContext`, and many modern Web APIs. Always mock explicitly in platform tests, or consider using Playwright for real browser testing.

### 2. **Rust Error Handling Across Platforms**

Keychain APIs return platform-specific error codes. macOS gives you `OSStatus`, iOS has its own set, and Android uses exceptions. Mapping these to consistent `Result<T>` types requires careful error translation:

```rust
impl From<security_framework::base::Error> for AurusError {
    fn from(err: security_framework::base::Error) -> Self {
        match err.code() {
            -25300 => AurusError::SecretNotFound,
            _ => AurusError::PlatformError(err.to_string()),
        }
    }
}
```

### 3. **Mobile Development Without the SDK**

Our development machine had Xcode Command Line Tools but not the full Xcode app—no iOS simulator. Android SDK wasn't installed either. This meant all mobile code was written and tested in "anticipation mode."

**Lesson**: Install your target platform SDKs early. `pnpm tauri ios init` and `pnpm tauri android init` are waiting, but they need the full toolchains.

## The Results

As of commit `16ce596`:
- ✅ **52/52 frontend tests passing** (3 test files covering platform detection)
- ✅ **7/7 Rust tests passing**
- ✅ **0 warnings** on `cargo check`
- ✅ **Clean builds** on `pnpm build`
- ✅ **37 files changed**: +5,205 additions, -92 deletions

The codebase is now structured for true cross-platform development, with clear boundaries between shared logic and platform-specific implementations.

## What's Next

The immediate roadmap:

1. **Phase 7**: Design Yjs + WebRTC sync for Desktop ↔ Mobile communication
2. Install full Xcode and test iOS simulator builds
3. Set up Android SDK and test Android builds
4. Create GitHub Actions workflows for mobile CI/CD
5. Test the Transformers.js worker in actual mobile browsers
6. Build the UI for switching between cloud and on-device transcription

## Takeaways for Cross-Platform Development

If you're building a cross-platform app with Tauri or similar frameworks:

1. **Start with abstractions**: Define your platform-agnostic interfaces first
2. **Embrace trait-based design**: Rust traits + TypeScript hooks = clean separation
3. **Test platform detection early**: Mock liberally in unit tests, but validate on real devices
4. **Don't skip the tooling**: Install SDKs and simulators before you need them
5. **Web APIs vary wildly**: What works in Chrome may not work in jsdom or mobile Safari

Building cross-platform is hard, but with the right architecture, it's incredibly rewarding. Each platform brings its own strengths, and a well-designed abstraction layer lets you leverage all of them.

---

*Aurus Voice Intelligence is an open-source voice transcription and analysis tool. The full source code and migration progress are available on [GitHub](#).*

*Have questions about cross-platform Tauri development? Found a better way to handle platform abstractions? I'd love to hear from you—drop a comment or reach out on Twitter.*