---
title: "Going Cross-Platform: Migrating a Desktop Voice App to Mobile"
date: 2026-02-05
tags: [tauri, rust, mobile-development, cross-platform, audio-processing, webrtc]
excerpt: "How we transformed Aurus Voice Intelligence from a desktop-only Tauri app into a true cross-platform experience, tackling audio pipelines, platform abstraction, and the quirks of Rust's thread safety along the way."
---

# Going Cross-Platform: Migrating a Desktop Voice App to Mobile

When we started building Aurus Voice Intelligence, the decision to use Tauri + Next.js made perfect sense for a desktop application. Fast, secure, and native-feeling—exactly what we needed. But as user requests for mobile support started rolling in, we faced a critical question: **Do we rebuild from scratch, or can we architect our way to cross-platform support?**

Spoiler alert: We chose the latter, and it's been quite the journey.

## The Challenge: Desktop-First Architecture Meets Mobile Reality

Our original architecture was beautifully simple for desktop:

```
CPAL (native audio) → Rust backend → Deepgram WebSocket → Transcription
```

The problem? CPAL (Cross-Platform Audio Library) doesn't support iOS or Android. Our audio pipeline was fundamentally desktop-only, and we needed a solution that could:

1. **Preserve the desktop experience** — no regressions
2. **Enable mobile audio capture** — using platform-appropriate APIs
3. **Share the transcription pipeline** — no duplicate logic
4. **Keep the codebase maintainable** — one app, not three

## Phase 1-2: Building the Foundation

### Platform Abstraction Layer

The first order of business was creating a platform abstraction layer. We needed our Rust code to handle secrets differently on each platform:

- **macOS/iOS**: Keychain
- **Windows**: Credential Manager  
- **Linux**: Secret Service
- **Android**: Keystore

We implemented a `SecureStorage` trait that let us write platform-agnostic code:

```rust
pub trait SecureStorage: Send + Sync {
    fn set(&self, key: &str, value: &str) -> Result<()>;
    fn get(&self, key: &str) -> Result<Option<String>>;
    fn delete(&self, key: &str) -> Result<()>;
}
```

This meant migrating away from our old plaintext JSON storage (yes, we know—it was a prototype!). The new system uses the OS's secure storage mechanisms, giving us proper security **and** a clean abstraction for platform differences.

### Frontend Platform Detection

On the frontend, we added a simple but crucial hook:

```typescript
// app/hooks/usePlatform.ts
export function usePlatform() {
  const [platform, setPlatform] = useState<Platform>('unknown');
  
  useEffect(() => {
    if (typeof window !== 'undefined') {
      if (window.__TAURI__) {
        // Detect desktop vs mobile within Tauri context
        setPlatform(detectTauriPlatform());
      } else {
        setPlatform('web');
      }
    }
  }, []);
  
  return platform;
}
```

Now our React components can adapt their behavior based on whether they're running on desktop, mobile, or web.

## Phase 3-4: The Audio Pipeline Puzzle

This is where things got interesting. We needed **two different audio capture strategies** feeding into **one transcription pipeline**.

### The Desktop Path

On desktop, we stick with CPAL for native audio capture:

```rust
// src-tauri/src/platform/audio/desktop.rs
pub struct DesktopAudioCapture {
    device: Device,
    config: StreamConfig,
}

impl AudioCapture for DesktopAudioCapture {
    fn start_capture(&mut self, callback: AudioCallback) -> Result<()> {
        let stream = self.device.build_input_stream(
            &self.config,
            move |data: &[f32], _: &_| {
                callback(AudioChunk::new(data.to_vec()));
            },
            // ... error handling
        )?;
        stream.play()?;
        Ok(())
    }
}
```

### The Mobile Path

On mobile, we leverage the Web Audio API through a React hook:

```typescript
// app/hooks/useWebAudioCapture.ts
export function useWebAudioCapture() {
  const startCapture = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ 
      audio: true 
    });
    
    const audioContext = new AudioContext({ sampleRate: 16000 });
    const source = audioContext.createMediaStreamSource(stream);
    const processor = audioContext.createScriptProcessor(4096, 1, 1);
    
    processor.onaudioprocess = (e) => {
      const audioData = e.inputBuffer.getChannelData(0);
      
      // Send directly to Rust via Tauri command
      invoke('send_audio_to_deepgram', { 
        audio: Array.from(audioData) 
      });
    };
    
    source.connect(processor);
    processor.connect(audioContext.destination);
  };
  
  return { startCapture, stopCapture };
}
```

### The Convergence Point

Both paths feed into the same Rust transcription pipeline:

```rust
#[tauri::command]
pub async fn send_audio_to_deepgram(
    audio: Vec<f32>,
    state: State<'_, AppState>
) -> Result<()> {
    // Desktop and mobile audio both arrive here
    let deepgram_ws = state.deepgram_connection.lock().await;
    deepgram_ws.send_audio_chunk(audio).await?;
    Ok(())
}
```

The architecture now looks like this:

```
┌─────────────────────────────────────────────────────────┐
│ Desktop: CPAL → Rust Audio Module → Transcription      │
└─────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────┐
│ Mobile: Web Audio → Tauri Command → Transcription      │
└─────────────────────────────────────────────────────────┘
                            ↓
                  ┌──────────────────────┐
                  │ Shared Deepgram WS   │
                  │ (transcription.rs)   │
                  └──────────────────────┘
```

### On-Device AI as a Bonus

Since we were already adapting for mobile constraints, we added on-device AI capabilities using Transformers.js running in a Web Worker:

```typescript
// app/workers/ai-worker.ts
import { pipeline } from '@xenova/transformers';

self.addEventListener('message', async (e) => {
  const { task, data } = e.data;
  
  switch (task) {
    case 'transcribe':
      const transcriber = await pipeline('automatic-speech-recognition', 
        'Xenova/whisper-tiny');
      const result = await transcriber(data);
      self.postMessage({ task, result });
      break;
    // ... summarize, analyze, etc.
  }
});
```

This gives us offline transcription and analysis capabilities—particularly useful for mobile users on spotty connections.

## Lessons Learned: Rust's Thread Safety Guardrails

### Challenge #1: The `Send + Sync` Dance

Our first attempt at the `AudioCapture` trait looked like this:

```rust
pub trait AudioCapture {
    fn start_capture(&mut self, 
        callback: Box<dyn Fn(AudioChunk) + Send>
    ) -> Result<()>;
}
```

Seems reasonable, right? The callback is `Send`, so we can move it between threads. But when we tried to wrap it in an `Arc` for shared access across multiple threads, Rust complained:

```
error: `dyn Fn(AudioChunk) + Send` cannot be shared between threads safely
```

**The fix:** Add `Sync` to the trait bound:

```rust
callback: Box<dyn Fn(AudioChunk) + Send + Sync>
```

**The lesson:** `Send` means "safe to move to another thread," but `Sync` means "safe to *share* across threads." When you're putting something in an `Arc`, you need both.

### Challenge #2: The Arc Clone Pattern

Later, we hit this pattern:

```rust
let is_recording = Arc::new(AtomicBool::new(true));

thread::spawn(move || {
    while is_recording.load(Ordering::Relaxed) {
        // ... work
    }
});

// Later...
is_recording.store(false, Ordering::Relaxed);  // ❌ Error: value moved
```

**The fix:** Clone the Arc before moving:

```rust
let is_recording = Arc::new(AtomicBool::new(true));
let is_recording_inner = Arc::clone(&is_recording);

thread::spawn(move || {
    while is_recording_inner.load(Ordering::Relaxed) {
        // ... work
    }
});

// Now this works!
is_recording.store(false, Ordering::Relaxed);  // ✅
```

**The lesson:** `Arc::clone()` is cheap (just increments a reference count) and essential whenever you need the same value both inside and outside a closure.

## Current Status and What's Next

We've completed the heavy lifting:

✅ Platform abstraction layer with secure storage  
✅ Migrated from plaintext to OS keychains  
✅ Cross-platform audio pipeline (desktop native + mobile web)  
✅ On-device AI infrastructure with Transformers.js  
✅ All tests passing (33 frontend, 7 Rust)

**Coming up:**

1. **Sync Architecture** — Implementing Yjs + WebRTC for real-time sync between desktop and mobile devices
2. **Mobile Testing** — Running actual builds on iOS simulator and Android emulator
3. **CI/CD Pipeline** — GitHub Actions workflows for mobile builds
4. **Performance Tuning** — Optimizing the Web Audio → Rust → WebSocket chain on mobile

## Why This Approach Works

Instead of maintaining separate codebases or limiting ourselves to the lowest common denominator, we built **adaptive architecture**:

- **Native where it matters** (audio on desktop, secure storage everywhere)
- **Web where it's pragmatic** (audio on mobile, AI inference)
- **Shared where it counts** (transcription pipeline, business logic)

The result? One codebase, multiple platforms, no compromises on quality.

---

**Want to follow along?** We'll be documenting the sync architecture implementation and mobile testing phases in upcoming posts. The challenges of real-time cross-device synchronization are... interesting, to say the least.

Have you tackled a similar desktop-to-mobile migration? What patterns worked for you? Let's chat in the comments!